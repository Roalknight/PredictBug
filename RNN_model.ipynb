{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNLSTM, Embedding\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(\"clean.json\") as f:\n",
    "    clean = json.load(f)\n",
    "with open(\"buggy.json\") as f:\n",
    "    buggy = json.load(f)\n",
    "with open(\"py2vec_modelJ.json\") as f:\n",
    "    embs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu trữ thành mảng, lúc này các đoạn mã vẫn là các chuỗi.\n",
    "clean = np.asarray(clean)\n",
    "buggy = np.asarray(buggy)\n",
    "# Tạo nhãn cho mỗi tiêu đề, 1 cho mã lỗi và 0 cho mã sạch\n",
    "buggy_labels = np.ones(len(buggy))\n",
    "clean_labels = np.zeros(len(clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "safemax\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Phần nhúng hiện được lưu trữ trong từ điển và cần biến nó thành một mảng 2 chiều có kích thước (số nút, độ dài của Embedding)\n",
    "# word_to_int sẽ lấy một nút và chuyển từ đó thành chỉ mục của từ đó thành ma trận nhúng\n",
    "# int_to_word được làm ngược lại - lấy một chỉ mục và chuyển đổi nó trở lại một nút\n",
    "embedding_matrix = []\n",
    "int_to_word = []\n",
    "word_to_int = {}\n",
    "i = 0\n",
    "for word, emb in embs.items():\n",
    "    embedding_matrix.append(emb)\n",
    "    int_to_word.append(word)\n",
    "    word_to_int[word] = i\n",
    "    i += 1\n",
    "    \n",
    "embedding_matrix.append(np.zeros(100)) # For unknown words we use an array of zeros.\n",
    "embedding_matrix = np.asarray(embedding_matrix)\n",
    "print(word_to_int['safemax'])\n",
    "print(int_to_word[2])\n",
    "print(np.array_equal(embs['safemax'], embedding_matrix[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sắp xếp lại data và nhãn\n",
    "train_data = np.concatenate((clean, buggy), axis=0)\n",
    "train_labels = np.concatenate((clean_labels, buggy_labels), axis=0)\n",
    "\n",
    "# Sắp xếp lại data\n",
    "for i in range(train_data.shape[0]):\n",
    "    string = ''\n",
    "    for j in range(len(train_data[i])):\n",
    "        string += train_data[i][j] + ' '\n",
    "    train_data[i] = string\n",
    "\n",
    "# Sử dụng hàm random để xáo trộn (shuffle) dữ liệu trong train_data và train_labels một cách ngẫu nhiên, \n",
    "# nhưng đảm bảo rằng sự tương ứng giữa các phần tử trong hai mảng này được duy trì.\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(train_data)\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy thử nghiệm 1000 mẫu lỗi \n",
    "test_data = train_data[train_data.shape[0]-1000:]\n",
    "test_labels = train_labels[train_labels.shape[0]-1000:]\n",
    "train_data = train_data[:train_data.shape[0]-1000]\n",
    "train_labels = train_labels[:train_labels.shape[0]-1000]\n",
    "\n",
    "num_words = len(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số lượng nút 497424\n",
      "Số nút lỗi 13988\n"
     ]
    }
   ],
   "source": [
    "# Chuyển đổi từng tiêu đề của chuỗi thành số nguyên - mỗi từ được biến thành chỉ mục của nó thành ma trận nhúng.\n",
    "train_data_tokens = []\n",
    "test_data_tokens = []\n",
    "num_words_missed = 0\n",
    "num_words_found = 0\n",
    "for i in range(train_data.shape[0]):\n",
    "    train_data_tokens.append([])\n",
    "    for word in train_data[i].split():\n",
    "        if word.lower() in embs:\n",
    "            train_data_tokens[i].append(word_to_int[word.lower()])\n",
    "            num_words_found += 1\n",
    "        else:\n",
    "            train_data_tokens[i].append(-1)\n",
    "            num_words_missed += 1\n",
    "for i in range(test_data.shape[0]):\n",
    "    test_data_tokens.append([])\n",
    "    for word in test_data[i].split():\n",
    "        if word.lower() in embs:\n",
    "            test_data_tokens[i].append(word_to_int[word.lower()])\n",
    "            num_words_found += 1\n",
    "        else:\n",
    "            test_data_tokens[i].append(embedding_matrix.shape[0]-1)\n",
    "            num_words_missed += 1\n",
    "print(\"Tổng số lượng nút %d\" % num_words_found)\n",
    "print(\"Số nút lỗi %d\" % num_words_missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_tokens[0])\n",
    "print(test_data_tokens[0])\n",
    "print(embedding_matrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2526, 2529, 4068, 4068, 5340, 994, 3984, 2561, 2905, 4603, 5918, 5626, 5934]\n",
      "_atsignsymbol_ gwtincompatible _divide_ _divide_ doublemath _dispatch_ roundtoint _openparen_ double _comma_ roundingmode _closeparen_ unknown\n"
     ]
    }
   ],
   "source": [
    "# Chuyển đổi một chuỗi các chỉ mục (tokens) trở lại thành một chuỗi văn bản ban đầu\n",
    "# Mục đích là để hiển thị lại tiêu đề của một văn bản trong tập huấn luyện dưới dạng chuỗi văn bản, \n",
    "# từ đó có thể đọc và kiểm tra kết quả của quá trình xử lý và huấn luyện mô hình\n",
    "print(train_data_tokens[2])\n",
    "int_to_word.append(\"unknown\")\n",
    "def tokens_to_string(tokens):\n",
    "    words = [int_to_word[token] for token in tokens if token != 0]\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "print(tokens_to_string(train_data_tokens[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tính tổng số token\n",
    "num_tokens = [len(tokens) for tokens in train_data_tokens + test_data_tokens]\n",
    "num_tokens = np.asarray(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.09547433989306"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9382613968358966"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(num_tokens < max_tokens) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 'pre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow yêu cầu mỗi title đều phải bằng nhau, tôi thực hiện việc căn chỉnh độ dài của các title thành một độ dài cố định\n",
    "train_data_pad = pad_sequences(train_data_tokens, maxlen=max_tokens,\n",
    "                              padding=pad, truncating=pad)\n",
    "test_data_pad = pad_sequences(test_data_tokens, maxlen=max_tokens,\n",
    "                             padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35282, 54)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,  400, 5404,\n",
       "       3580, 2298, 2561, 3877, 4603, 1649, 5626, 4029,  994, 5934],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_data_pad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/paperspace/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# Create Network\n",
    "from keras.layers import Dropout\n",
    "num_words = len(int_to_word)\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                   output_dim=embedding_matrix.shape[1],\n",
    "                   input_length=max_tokens,\n",
    "                   weights=[embedding_matrix],\n",
    "                   trainable=False,\n",
    "                   name='embedding_layer'))\n",
    "model.add(CuDNNLSTM(16, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(CuDNNLSTM(8))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizer,\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 54, 100)           593500    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 54, 16)            7552      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 54, 16)            0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 8)                 832       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 601,893\n",
      "Trainable params: 8,393\n",
      "Non-trainable params: 593,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33517 samples, validate on 1765 samples\n",
      "Epoch 1/10\n",
      "33517/33517 [==============================] - 8s 237us/step - loss: 0.6935 - acc: 0.5052 - val_loss: 0.6898 - val_acc: 0.5439\n",
      "Epoch 2/10\n",
      "33517/33517 [==============================] - 6s 168us/step - loss: 0.6155 - acc: 0.6462 - val_loss: 0.5089 - val_acc: 0.7275\n",
      "Epoch 3/10\n",
      "33517/33517 [==============================] - 6s 172us/step - loss: 0.4859 - acc: 0.7485 - val_loss: 0.4309 - val_acc: 0.7841\n",
      "Epoch 4/10\n",
      "33517/33517 [==============================] - 6s 170us/step - loss: 0.4358 - acc: 0.7776 - val_loss: 0.3932 - val_acc: 0.7977\n",
      "Epoch 5/10\n",
      "33517/33517 [==============================] - 6s 168us/step - loss: 0.4088 - acc: 0.7922 - val_loss: 0.3757 - val_acc: 0.8062\n",
      "Epoch 6/10\n",
      "33517/33517 [==============================] - 6s 166us/step - loss: 0.3913 - acc: 0.7984 - val_loss: 0.3649 - val_acc: 0.8125\n",
      "Epoch 7/10\n",
      "33517/33517 [==============================] - 6s 168us/step - loss: 0.3741 - acc: 0.8056 - val_loss: 0.3518 - val_acc: 0.8113\n",
      "Epoch 8/10\n",
      "33517/33517 [==============================] - 6s 169us/step - loss: 0.3615 - acc: 0.8129 - val_loss: 0.3363 - val_acc: 0.8278\n",
      "Epoch 9/10\n",
      "33517/33517 [==============================] - 6s 168us/step - loss: 0.3538 - acc: 0.8152 - val_loss: 0.3344 - val_acc: 0.8204\n",
      "Epoch 10/10\n",
      "33517/33517 [==============================] - 6s 169us/step - loss: 0.3416 - acc: 0.8224 - val_loss: 0.3223 - val_acc: 0.8300\n",
      "CPU times: user 1min 6s, sys: 7.63 s, total: 1min 14s\n",
      "Wall time: 59.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fead2842cf8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "%%time\n",
    "model.fit(train_data_pad, train_labels,\n",
    "         validation_split=0.05, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 152us/step\n"
     ]
    }
   ],
   "source": [
    "# Test trên testing data\n",
    "result = model.evaluate(test_data_pad, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 80.10%\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
